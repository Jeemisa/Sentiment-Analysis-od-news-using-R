---
title: "News headline- Sentiment Analysis"
author: "XYZ"
date: "May 05, 2019"
output: html_document
---
Headlines make a huge difference in how readers perceive news and newspapers,especially if they’re voracious or busy speed-readers.
Headlines should be such that they lure readers to dive into the article. 
The focus in on grabing the attention of the reader and drawing him into the story.
Let's analyse the sentiments hidden in the headlines of Australian Broadcasting Corp (ABC).

##Necessary Packages and Libraries

```{r message=FALSE, warning=FALSE}

#Installing packages

install.packages("tm")  # for text mining
install.packages("SnowballC") # for text stemming
install.packages("wordcloud") # word-cloud generator 
install.packages("RColorBrewer") # color palettes
install.packages("sentimentr")
install.packages("tidytext")

# Importing Libraries

library('syuzhet') #for sentiment analysis
library(readr)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(SnowballC)
library(sentimentr)
library(tidytext)
library(stringi)

```

##Importing dataset 
```{r, message=FALSE, warning=FALSE}
abcnews = read_csv("abcnews-date-text.csv")

```

## Sentiment Analysis

```{r, message=FALSE, warning=FALSE}

#Getting year from publish_date
abcnews$year = substr(abcnews$publish_date, start = 1, stop = 4)

dim(abcnews)
class(abcnews)

abcnews1 = abcnews %>%
select(year, headline_text,publish_date) %>%
 filter(year == c(2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017))

```
##Using NRC Lexicon
```{r, message=FALSE, warning=FALSE}

d = get_nrc_sentiment(abcnews1$headline_text)
td = data.frame(t(d))
length(td)


td_new = data.frame(rowSums(td[2:length(td)]))     

#Data Preparation

names(td_new)[1] = "count"
td_new = cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) = NULL
td_new2 = td_new[1:10,]

#Vizualisation

qplot(sentiment, data=td_new2, weight=count, geom="bar",fill=sentiment)+ggtitle("Abc News headlines sentiment analysis")

```

```{r, message=FALSE, warning=FALSE, echo = FALSE}


bp<- ggplot(td_new2, aes(x=sentiment, y=count, fill=td_new2$sentiment))+
geom_bar(width = 1, stat = "identity")
bp+ coord_polar("x", start=0) + theme(legend.position="none")+labs( x = "", y = "")+theme(axis.text=element_text(size=12),
        axis.title=element_text(size=12,face="bold"))

```


```{r include=FALSE}
options(tibble.width = Inf)
```

##Year-wise analysis{.tabset}

#### Major sentiments across year 2003 

```{r, result='asis', echo = FALSE}
abcnews1 <- abcnews %>%
select(year, headline_text) %>%
 filter(year == 2003)

d<-get_nrc_sentiment(abcnews1$headline_text)
td<-data.frame(t(d))

td_new <- data.frame(rowSums(td[2:7000]))     

#Transformation and  cleaning
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:10,]

qplot(sentiment, data=td_new2, weight=count, geom="bar",fill=sentiment)+ggtitle("Abc News headlines sentiment analysis for 2003")


bp<- ggplot(td_new2, aes(x=sentiment, y=count, fill=td_new2$sentiment))+
geom_bar(width = 1, stat = "identity")
bp+ coord_polar("x", start=0) + theme(legend.position="none")+labs( x = "", y = "")+theme(axis.text=element_text(size=12),
        axis.title=element_text(size=12,face="bold"))

 
```

#### Major sentiments across year 2004

```{r, result='asis',  echo = FALSE}
abcnews1 <- abcnews %>%
select(year, headline_text) %>%
 filter(year == 2004)

d<-get_nrc_sentiment(abcnews1$headline_text)
td<-data.frame(t(d))

td_new <- data.frame(rowSums(td[2:7000]))     

#Transformation and  cleaning
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:10,]

qplot(sentiment, data=td_new2, weight=count, geom="bar",fill=sentiment)+ggtitle("Abc News headlines sentiment analysis for 2004")


bp<- ggplot(td_new2, aes(x=sentiment, y=count, fill=td_new2$sentiment))+
geom_bar(width = 1, stat = "identity")
bp+ coord_polar("x", start=0) + theme(legend.position="none")+labs( x = "", y = "")+theme(axis.text=element_text(size=12),
        axis.title=element_text(size=12,face="bold"))
        
```

#### Major sentiments across year 2005

```{r, result='asis', echo = FALSE}
abcnews1 <- abcnews %>%
select(year, headline_text,publish_date) %>%
 filter(year == 2005)

d<-get_nrc_sentiment(abcnews1$headline_text)
td<-data.frame(t(d))

td_new <- data.frame(rowSums(td[2:7000]))     

#Transformation and  cleaning
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:10,]


qplot(sentiment, data=td_new2, weight=count, geom="bar",fill=sentiment)+ggtitle("Abc News headlines sentiment analysis for 2005")


bp<- ggplot(td_new2, aes(x=sentiment, y=count, fill=td_new2$sentiment))+
geom_bar(width = 1, stat = "identity")
bp+ coord_polar("x", start=0) + theme(legend.position="none")+labs( x = "", y = "")+theme(axis.text=element_text(size=12),
        axis.title=element_text(size=12,face="bold"))
        
 
```

##Using stringi

```{r, message=FALSE, warning=FALSE}

stri_sub(abcnews1$publish_date, 5, 4) <- "-"
stri_sub(abcnews1$publish_date, 8, 7) <- "-"


```        

##Positive sentiment
 
```{r, echo=FALSE, message=FALSE, warning=FALSE}

#We calculate the weighted average of sentiments per day
dataf <- as.data.frame(sentiment_by(abcnews1$headline_text))

names(dataf)

#Join tables 

abcnews1$ID<-seq.int(nrow(abcnews1))


df<-merge(abcnews1, dataf, by.x="ID", by.y="element_id")

df$Date2<-as.Date(df$publish_date)


p<- ggplot(df, aes(x=Date2, y=ave_sentiment))+geom_line()+geom_hline(yintercept=0, colour="red", size=1.5)+ labs(x="Time",y="Emotional valence") 

p 


```

##Creating word-cloud

```{r, message=FALSE, warning=FALSE}
library(tm)
library(wordcloud)
makeWordCloud <- function(documents) {
  corpus = Corpus(VectorSource(tolower(documents)))
  corpus = tm_map(corpus, removePunctuation)
  corpus = tm_map(corpus, removeWords, stopwords("english"))
  
  frequencies = DocumentTermMatrix(corpus)
  word_frequencies = as.data.frame(as.matrix(frequencies))
  
  words <- colnames(word_frequencies)
  freq <- colSums(word_frequencies)
  wordcloud(words, freq,
            min.freq=sort(freq, decreasing=TRUE)[[100]],
            colors=brewer.pal(8, "Dark2"),
            random.color=TRUE) 
}  

makeWordCloud(abcnews[["headline_text"]][1:2000])

```
##Most frequently used words

```{r, message=FALSE, warning=FALSE}
library(tidytext)

title_words <- abcnews %>%
   select(publish_date, headline_text) %>%
    unnest_tokens(Word , headline_text)

title_word_counts <- title_words %>%
    anti_join(stop_words, c(Word = "word")) %>%
    count(Word, sort = TRUE)

title_word_counts %>%
    head(20) %>%
    mutate(Word = reorder(Word, n)) %>%
    ggplot(aes(Word, n)) +
    geom_bar(stat = "identity") +
    ylab("Number of appearances in R question titles") +
    coord_flip()


```
##Adding month and year to dataset

Here we can see that the publish_date is an integer column and the headline_text has a factor data type. We need to change the data type for both the columns. Publish_date should be of date data type and headline_text should be of the string datatype. Let's do it. We will also add columns for month and year to the data for further analysis and then convert year to factor variable. 

```{r, message= FALSE, warning= FALSE}
str(abcnews)

abcnews$publish_date<-as.Date(as.character(abcnews$publish_date),format="%Y%m%d")
abcnews$headline_text<-as.character(abcnews$headline_text)

str(abcnews)

abcnews$year<-format(abcnews$publish_date,"%Y")
abcnews$month<-format(abcnews$publish_date,"%m")

abcnews$year<-as.factor(abcnews$year)

```
We will use different lexicons like bing, nrc, afinn. Each of these lexicons is different in its own. They help in scoring the words present in the text.

Bing lexicon will categorizes words in the text into 2 categories i.e. Positive or Negative

nrc lexicon categorizes words into emotions like anger, disgust, fear, joy, etc.;

AFINN lexicon categorizes words using a score, with negative scores indicating a negative sentiment.

Note: Lexicons do not take into account qualifiers before a word, such as in “no good” or “not true”.

##Unnest token to split the phrases into single words

```{r, message=FALSE, warning=FALSE}
names(abcnews)
abc_words= abcnews %>% unnest_tokens(word,headline_text)

#understanding stop words
head(stop_words)
```
Then next step after applying unnest tokens would be to use anti_join to ignore stopwords from the data. Since stop words don't cause any significant impact in sentiment analysis. But before that we will just have a look at what are stop words and then proceed further to remove them from our dataset. So as seen above smart words are simply words which inherently don't have any sentiment whatsoever. Now I will proceed to remove these words from teh dataset.

```{r}
abc_sentiment<-abc_words%>%
  anti_join(stop_words,by="word")
```
##Top 20 words that have been in the headlines for most number of times 
```{r, message=FALSE, warning= FALSE}

abc_sentiment%>%count(word,sort=T)%>%top_n(20)

#Visualizing

ggplot(abc_sentiment%>%count(word,sort=T)%>%top_n(20),aes(reorder(word,n),n))+
  geom_bar(stat = "identity")+
  geom_text(aes(label = n),color="#0f190f", hjust = -0.05, size = 2)+
  theme_bw()+
  coord_flip()+
  xlab("Number of Occurences")+
  ylab("Words used")+
  ggtitle("Number of Occurences of each word")+
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
```
##TOP 3 words across different years in the dataset

```{r, message=FALSE, warning=FALSE}
ggplot(abc_sentiment%>%group_by(year,word)%>%summarize(num=n())%>%top_n(3,num)%>%ungroup(),
       aes(x=reorder(word,num),y=num))+
  geom_bar(stat = "identity")+
  theme_bw()+
  coord_flip()+
  xlab("Number of Occurences")+
  ylab("Words used")+
  ggtitle("Number of Occurences of each word")+
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_text(angle = 40, hjust = 1))+
  facet_wrap(~year,scales = "free")
```
The word "Interview" seems to be a bit surprising in the above visualization. Let's add this word to the stopwords list and re-create the above bar-charts to look into changes. 

```{r}
stopwords<-data.frame(word=c(stop_words$word,"interview"))

stopwords$word<-as.character(stopwords$word)

abc_sentiment_stopwords<-abc_words%>%
  anti_join(stopwords,by="word")

#Visualizing

ggplot(abc_sentiment_stopwords%>%group_by(year,word)%>%summarize(num=n())%>%top_n(3,num)%>%ungroup(),
       aes(x=reorder(word,num),y=num))+
  geom_bar(stat = "identity")+
  theme_bw()+
  coord_flip()+
  xlab("Number of Occurences")+
  ylab("Words used")+
  ggtitle("Number of Occurences of each word")+
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_text(angle = 40, hjust = 1))+
  facet_wrap(~year,scales = "free")

```
##Using BING Lexicon

```{r}

abc_bing<-abc_sentiment_stopwords%>%
  inner_join(get_sentiments("bing"),by="word")%>%
  ungroup()

head(abc_bing)

#TOP 10 positve and negative words in the headlines using BING lexicon

abc_bing%>%
  count(word,sentiment)%>%
  group_by(sentiment)%>%
  top_n(10,n)%>%
  ungroup()%>%
  ggplot(aes(x=reorder(word,n),y=n,fill=sentiment))+
  geom_col(show.legend = FALSE)+
  coord_flip()+
  facet_wrap(~sentiment,scales="free")+
  labs(x="number of occurences",y="Words",title="Top 10 positive and negative sentiment words used in headlines using bing lexicon")+
  theme(plot.title = element_text(size = 8, face = "bold"))

```
##Using NRC Lexicon

This will classify words into different emotions like fear, anger, happiness, disgust etc.
Let's see the top 5 words for each emotion.

```{r, message=FALSE, warning= FALSE}
abc_nrc<-abc_sentiment_stopwords%>%
  inner_join(get_sentiments("nrc"),by="word")%>%
  ungroup()
table(abc_nrc$sentiment)

abc_nrc%>%
  filter(sentiment %in% c("joy","anger","positive","negative","sadness"))%>%
  group_by(sentiment)%>%
  count(word,sentiment)%>%
  top_n(10,n)%>%
  ungroup()%>%
  
  ggplot(aes(x=reorder(word,n),y=n,fill=sentiment))+
  geom_col(show.legend=F)+
  coord_flip()+
  facet_wrap(~sentiment,scales="free")+
  labs(x="number of occurences",y="Words",title="Top 10 words for each emotion used in headlines using nrc lexicon")+
  theme(plot.title = element_text(size = 8, face = "bold"))
```
##Using AFINN Lexicon
This lexicon scores each word with a scale of -5 to 5. -5 means a very negative word where as 5 indicates a positive word.

```{r, message=FALSE, warning=FALSE}
abc_afinn<-abc_sentiment_stopwords%>%
  inner_join(get_sentiments("afinn"),by="word")%>%
mutate(sentiment=ifelse(score<0,'Negative','Positive'))

table(abc_afinn$score,abc_afinn$sentiment)

#TOP 10 negative and positive words in the news headlines.

abc_afinn%>%
group_by(sentiment)%>%
count(word,sentiment)%>%
top_n(10,n)%>%
ungroup()%>%
ggplot(aes(x=reorder(word,n),y=n,fill=sentiment))+
geom_col(show.legend=F)+
facet_wrap(~sentiment,scales="free")+
coord_flip()+
labs(x="number of occurences",y="Words",title="Top 10 words for each sentiment used in headlines using afinn lexicon")+
theme(plot.title = element_text(size = 8, face = "bold"))
```

##Thank You!
